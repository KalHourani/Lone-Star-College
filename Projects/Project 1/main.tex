\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{minted}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Project 1}
\author{Khalid Hourani}

\begin{document}
\maketitle
\section{Analysis Tools}
\begin{itemize}
  \item The Constant Function

  A constant function is any function of the form \[f(n) = c\] where $c$ is some constant. Since any constant function $f(n) = c$ can be written as \[f(n) = c \cdot g(n)\] where $g(n) = 1$ is another constant function, we will typically focus on the constant function $f(n)=1$. The constant function frequently describes the number of steps required for basic operations, like assigning a value to a variable, adding two integers, or accessing an array index. 

  \item The Logarithm Function

  A logarithm function is any function of the form \[f(n)=\log_b(n)\] Defined by \[\log_b(x)=y\text{ if and only if } b^y=x\] Typically, while mathematicians write $\log$ to mean $\ln=\log_e$ and engineers and scientists write $\log$ to mean $\log_{10}$, computer scientists write $\log$ to mean $\log_2$. However, any base-b logarithm can be converted to base-a by the formula \[\log_b(n)=\frac{\log_a(n)}{\log_a(b)}\] so the choice of base has little impact in practice. The following four properties of logarithms are notable:
  \begin{enumerate}
  \item $\log_b(mn) = \log_b(m)+\log_b(n)$
  \item $\log_b(m/n) = \log_b(m) - \log_b(n)$
  \item $\log_b(m^n) = n\log_b(m)$
  \item $b^{\log_b(m)} = m$
  \end{enumerate}

  The logarithm function frequently shows up in algorithm analysis in examples like the binary search, which, on a search of $n$ elements, takes at most $\log(n)$ operations.
  \item The Linear Function

  A linear function is any function of the form \[f(n)=cn\] This function frequently occurs when we have to perform an operation on each element in a set. For example, a linear search on $n$ elements takes at most $n$ comparison operations.

  \item The N-Log-N Function

  The N-Log-N function is a function of the form \[f(n)=n\log(n)\] This function grows faster than the logarithm function and the linear function, but slower than the quadratic function. This function tends to show up when optimizing a function that runs in quadratic time.
  \item The Quadratic Function

  A quadratic function is any polynomial \[f(n) = an^2+bn+c\] However, we typically focus on the quadratic \[f(n) = n^2\] since \[an^2+bn+c\leq\left(|a|+|b|+|c|\right)n^2\] This function tends to show up when nesting loops. For example, multiplying two numbers using the elementary algorithm requires quadratic time, since you must iterate through each digit of the first number for each iteration through each digit of the second.

  \item The Cubic Function and Other Polynomials

  The cubic function is any polynomial \[f(n)=an^3+bn^2+cn+d\] Though, as in the above example, we typically consider \[f(n) = n^3\] A polynomial function of degree $m$ is a function of the form \[f(n)=a_0n^m+a_1n^{m-1}+\cdots+a_{n-1}n+a_m\] Similarly, we typically consider the polynomial \[f(n)=n^m\] These functions show up in cases where there are multiple nested loops. In the case of four nested loops, for example, the function would have a running time of approximately $n^4$.

  \item The Exponential Function

  An exponential function is any function of the form \[f(n)=b^n\] The exponential function can be defined for non-negative integer values recursively as \[b^n=\begin{cases}1 & n = 0\\b\cdot b^{n-1} & n>0\end{cases}\] This definition can be extended to all integers by setting \[b^{-n}=\frac{1}{b^n}\] and to all rational numbers by \[b^{\frac{m}{n}}=\sqrt[n]{b^m}\] Finally, we can extend the definition to all real numbers by defining \[b^r=\lim_{n\to\infty}b^{r_n}\] where $r_n$ is any sequence of rationals such that $r_n\to r$. An equivalent definition is found by setting \[\ln(x)=\int_1^x\frac{1}{t}dt\] and defining $e^x$ as the inverse function of $\ln{x}$. We then set \[b^x=e^{x\ln{b}}\]

  The following three properties are notable:
  \begin{enumerate}
  \item $b^m\cdot b^n=b^{m+n}$
  \item $\frac{b^m}{b^n}=b^{m-n}$
  \item $(b^m)^n=b^{mn}$
  \end{enumerate}

The exponential function frequently shows up when something is repeatedly doubled.
\end{itemize}

\section{Asymptotes}
\begin{itemize}
\item Asymptotic Notation

Asymptotic Notation is the use of the following terminology to describe the behavior of an algorithm:
\begin{enumerate}
\item $O(n)$ - \textbf{Big O Notation}
\item $\Omega(n)$ - \textbf{Big Omega Notation}
\item $\Theta(n)$ - \textbf{Big Theta Notation}
\end{enumerate}

We say a function $f(n)$ is $O(g(n))$, written \begin{align*}f(n) &\text{ is } O(g(n))\text{ or }\\f(n)&=O(g(n))\text{ or }\\f(n)&\in O(g(n))\end{align*} if and only if there exists a real constant $c>0$ and an integer constant $m\geq1$ such that \[f(n)\leq cg(n) \text{ for all }n\geq m\]

For example, the function $f(n)=n^2+4n+9$ is $O(n^3)$, since \[n^2+4n+9\leq 1n^3\text{ for } n\geq4\]

Similarly, we say that $f(n)$ is $\Omega(g(n))$, written \begin{align*}f(n) &\text{ is } \Omega(g(n))\text{ or }\\f(n)&=\Omega(g(n))\text{ or }\\f(n)&\in \Omega(g(n))\end{align*} if and only if there exists a real constant $c>0$ and an integer constant $m\geq1$ such that \[f(n)\geq cg(n) \text{ for all }n\geq m\] Equivalently, $f(n)$ is $\Omega(g(n))$ if and only if $g(n)$ is $O(f(n))$. 

For example, the function $f(n)=n^2+4n+9$ is $\Omega(n)$, since \[n^2+4n+9\geq 1n\text{ for }n\geq1\]

Finally, we say that $f$ is $\Theta(g(n))$, written \begin{align*}f(n) &\text{ is } \Theta(g(n))\text{ or }\\f(n)&=\Theta(g(n))\text{ or }\\f(n)&\in \Theta(g(n))\end{align*} if and only if there exist real constants $c'>0$ and $c''>0$ and an integer constant $m\geq1$ such that \[c'g(n) \leq f(n)\leq c''g(n) \text{ for all }n\geq m\] Equivalently, $f(n)=\Theta(g(n))$ if and only if $f(n)=O(g(n))$ \textbf{and} $f(n)=\Omega(g(n))$

For example, the function $f(n)=n^2+4n+9$ is $\Theta(n^2)$, since \[n^2 \leq n^2+4n+9\leq14n^2\text{ for } n\geq1\]

In practice, Big $O$ notation is more frequently used than Big $\Omega$ and Big $\Theta$, since we are often more concerned with a ``worst case'' estimation of the performance and memory space of an algorithm.

\item Asymptotic Analysis

Asymptotic Analysis is the use of the above three definitions to analyze the speed and memory-efficiency of an algorithm. If a function $f(n)$ is $O(g(n))$, then, for sufficiently large $n$ (hence ``asymptotic''), we know that $f$ is no slower than $g$. For example, if a particular algorithm runs in $O(n^2)$ time, then we know that doubling the size of the input will at most \textbf{quadruple} the runtime. Consider the following algorithm for determining primality of an integer $n$:

\begin{minted}{C++}
bool isPrime(int n)
{
    if (n == 1)
    {
        return false;
    }
    else if (n == 2)
    {
        return true;
    }
    else if (n == 3)
    {
        return true;
    }
    else
    {
        for (int i = 2; i * i <= n; i++)
    {
        if (n % i == 0)
        {
            return false;
        }
    }
    return true;
    }
}
\end{minted}

We see that this algorithm will perform at most $\sqrt{n}$ operations (when $n$ is a perfect-square or prime), and is therefore $O(\sqrt{n})$. In other words, if we quadruple the input, we will require at most \textbf{double} the number of operations. However, in many cases we will not \textbf{actually} require double the number of operations. Consider the function evaluated on $n=4$. The function will perform one operation before returning false, simply testing divisibility by 2. When we evaluate the function on $16$, it similarly only takes only a single operation. In fact, this best-case scenario (when $n$ is even) shows that the algorithm is $\Omega(1)$. 

Similarly, consider, the following function to recursively evaluate $x^n$:
\begin{minted}{C++}
int pow(double x, int n)
{
	if (n == 0)
	{
		return 1;
	}
	else
    {
		return x * pow(x, n - 1);
    }
}
\end{minted}

This function will perform \textbf{exactly} $n$ operations, and therefore is $\Theta(n)$ (and therefore $O(n)$). However, this function can be improved by performing what is called ``binary exponentiation''. For example, in order to calculate $x^9$, we write $9=1001_b$ and observe then that\begin{align*}x^9&=x^{1001_b}\\&=x^{1\cdot2^3+0\cdot2^2+0\cdot2^1+1\cdot2^0}\\&=x^{1\cdot2^3}x^{0\cdot2^2}x^{0\cdot2^1}x^{1\cdot2^0}\\&=x^8x^1\end{align*} This creates the far more efficient algorithm:

\begin{minted}{C++}
int pow(double x, int n)
{
	int result = 1;
	while (exponent > 0)
	{
		if (exponent % 2 == 1)
		{
			result *= base;
		}
		exponent /= 2;
		base *= base;
	}
	return result;
}
\end{minted}

We see that this version of the function will terminate when the exponent becomes 0, which will occur after integer division by 2 has occurred $\lceil\log(n)\rceil$ times. Thus, this version of the function is $O(\log(n))$.

As a rule of thumb, algorithms which are ``faster'' asymptotically are preferred. So an $O(n)$ algorithm is preferable to an $O(n^2)$ algorithm. This isn't \textit{always} true, however, as an algorithm with an $O(10^{1000}n)$ runtime, while certainly faster in the long-term than an $O(n^2)$ algorithm, is hardly preferable. In fact, the $O(10^{1000}n)$ runtime algorithm will not see such results until $n>10^{1000}$. In other words, it is important to contextualize these bounds.
\end{itemize}

\section{Queue Operations}
The following queue operations:

enqueue(5), enqueue(3), dequeue(), enqueue(2), enqueue(8), dequeue(), dequeue(), enqueue(9), enqueue(1), dequeue(), enqueue(7), enqueue(6), dequeue(), dequeue(), enqueue(4), dequeue(), dequeue()

appear as follows:

\begin{align*}
\boxed{\phantom{5}}&\xrightarrow{\text{enqueue(5)}}\boxed{5}\xrightarrow{\text{enqueue(3)}}\boxed{3}\boxed{5}\xrightarrow{\text{dequeue()}}\boxed{3}\xrightarrow{\text{enqueue(2)}}\boxed{2}\boxed{3}\xrightarrow{\text{enqueue(8)}}\boxed{8}\boxed{2}\boxed{3}\\&\xrightarrow{\text{dequeue()}}\boxed{8}\boxed{2}\xrightarrow{\text{dequeue()}}\boxed{8}\xrightarrow{\text{enqueue(9)}}\boxed{9}\boxed{8}\xrightarrow{\text{enqueue(1)}}\boxed{1}\boxed{9}\boxed{8}\xrightarrow{\text{dequeue()}}\boxed{1}\boxed{9}\\&\xrightarrow{\text{enqueue(7)}}\boxed{7}\boxed{1}\boxed{9}\xrightarrow{\text{enqueue(6)}}\boxed{6}\boxed{7}\boxed{1}\boxed{9}\xrightarrow{\text{dequeue()}}\boxed{6}\boxed{7}\boxed{1}\xrightarrow{\text{dequeue()}}\boxed{6}\boxed{7}\\&\xrightarrow{\text{enqueue(4)}}\boxed{4}\boxed{6}\boxed{7}\xrightarrow{\text{dequeue()}}\boxed{4}\boxed{6}\xrightarrow{\text{dequeue()}}\boxed{4}
\end{align*}
\end{document}
